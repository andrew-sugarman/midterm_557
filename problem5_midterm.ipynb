{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "References:\n",
    "- https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\n",
    "- Elements of Statistical Learning\n",
    "- https://sassafras13.github.io/MLEvsMAP/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable for problem\n",
    "- Define a prior probability for beta given some parameters such that the maximum a posteriori estimator for beta is equal to beta for ols\n",
    "\n",
    "### Steps to solve \n",
    "- We know from the definition of maximum likelihood estimation that the MLE estimator for beta in linear regression is equal to the OLS estimator for beta \n",
    "- Maximum a posteriori (MAP estimation) is a generalization of maximum likelihood estimation such that a prior is placed on the estimand\n",
    "- The problem asks us to define this prior in such a way that MAP = MLE for OLS\n",
    "- Therefore, we must define the prior such that the MAP equation reduces to the MLE equation\n",
    "- To solve this, I will use a uniform prior for beta (at p(B|?) = 1) so that the MAP equation is left with the likelihood function for MLE:\n",
    "\n",
    "We are given:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{OLS}} = \\arg\\min_{\\beta} \\sum_i \\left( y_i - \\beta^T x_i \\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\beta^T x_i, \\sigma^2) \\\\\n",
    "\\beta \\sim p(\\cdot)\n",
    "$$\n",
    "\n",
    "For $\\hat{\\beta}_{\\text{MAP}}$ we are given:\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg\\max_{\\beta} \\prod_{i=1}^{N}{ \\sim \\mathcal{N}(y_i | \\beta^T x_i, \\sigma^2)p(\\beta | \\cdot) }\n",
    "$$\n",
    "\n",
    "Where $p$ is a prior distribution with arbitrary params $\\cdot$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg \\max_{\\beta} P(\\beta | \\{x_i, y_i\\}) \n",
    "$$\n",
    "\\\\\n",
    "$\\textbf{To solve:}$\n",
    "\\\\\n",
    "We know from class and wikipedia that the solution to OLS with normally distributed residuals is MLE (if we minimize the negative log likelihood). We also know that MAP is a generalization for MLE in which we place a prior on our parameters. In the case of normally distributed residuals that give us the first equation, we know that we need to set a prior such that MAP is equal to MLE. \n",
    "\n",
    "Setting the prior $p(\\beta | \\cdot) = 1$ allows the prior to be ignored in the sampling of the posterior for $\\hat{\\beta}$. Thus MAP is = MLE and we already know that MLE with normally distributed residuals is equal to minimizing the sum of squared residuals. \n",
    "\n",
    "So, we set a uniform prior at 1 so that the prior can be ignored and the posterior simplified to MLE: \n",
    "\n",
    "$$\n",
    "p(\\beta | \\cdot) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\hat{\\beta}_{\\text{OLS}} = \\arg\\min_{\\beta} \\sum_i \\left( y_i - \\beta^T x_i \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\beta | \\{x_i, y_i\\}) = \\prod_{i=1}^{n} P(y_i | x_i, \\beta)\n",
    "$$\n",
    "\n",
    "Thus, we can rewrite MAP estimation of $\\beta$ as the following:\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg \\max_{\\beta} L(\\beta | \\{x_i, y_i\\}) P(\\beta)\n",
    "$$\n",
    "\n",
    "And: \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg \\max_{\\beta} P(\\beta | \\{x_i, y_i\\}) \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Taking a uniform prior = 1 over beta we can write that $\\hat{\\beta}_{\\text{OLS}} = \\hat{\\beta}_{\\text{MAP}}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg \\max_{\\beta} L(\\beta | \\{x_i, y_i\\}) * 1\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
